{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6dbf16-9647-4c2e-b56d-aa46b8f07575",
   "metadata": {},
   "source": [
    "## Checking PyTorch and CUDA Availability\n",
    "\n",
    "In this code cell, we import the PyTorch library and print out information about the availability of CUDA, the CUDA version, and the PyTorch version. This is useful for verifying that PyTorch is correctly installed and that it can utilize the GPU for computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5cc3408-19a9-478a-bf26-95366279432a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:31:42.236517Z",
     "iopub.status.busy": "2025-11-21T15:31:42.236284Z",
     "iopub.status.idle": "2025-11-21T15:31:42.676311Z",
     "shell.execute_reply": "2025-11-21T15:31:42.675858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "CUDA Version: None\n",
      "PyTorch Version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available()) \n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748a4c5-3471-47da-a629-b82fd1ac541a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading YOLOv5 Model\n",
    "\n",
    "In this code cell, we load the YOLOv5 model from a local directory using PyTorch's hub module. After loading the model, a confirmation message is printed to indicate that the YOLOv5 model has been successfully loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db82d40c-2db5-4ccd-a187-21664413d9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:31:42.677363Z",
     "iopub.status.busy": "2025-11-21T15:31:42.677268Z",
     "iopub.status.idle": "2025-11-21T15:31:43.992167Z",
     "shell.execute_reply": "2025-11-21T15:31:43.991787Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m PROJECT_ROOT = Path().resolve().parent\n\u001b[32m      4\u001b[39m YOLOV5_REPO = PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33myolov5_src\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mYOLOV5_REPO\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolov5n\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYOLOv5 model loaded successfully from\u001b[39m\u001b[33m\"\u001b[39m, YOLOV5_REPO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:652\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    643\u001b[39m     repo_or_dir = _get_cache_or_reload(\n\u001b[32m    644\u001b[39m         repo_or_dir,\n\u001b[32m    645\u001b[39m         force_reload,\n\u001b[32m   (...)\u001b[39m\u001b[32m    649\u001b[39m         skip_validation=skip_validation,\n\u001b[32m    650\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m model = \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:682\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[32m    681\u001b[39m     hubconf_path = os.path.join(hubconf_dir, MODULE_HUBCONF)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     hub_module = \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m    685\u001b[39m     model = entry(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:115\u001b[39m, in \u001b[36m_import_module\u001b[39m\u001b[34m(name, path)\u001b[39m\n\u001b[32m    113\u001b[39m module = importlib.util.module_from_spec(spec)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec.loader, Loader)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:758\u001b[39m, in \u001b[36m_LoaderBasics.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexec_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[32m    757\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the module.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    760\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcannot load module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m when \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    761\u001b[39m                           \u001b[33m'\u001b[39m\u001b[33mget_code() returns None\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:895\u001b[39m, in \u001b[36mSourceLoader.get_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n\u001b[32m    891\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _compile_bytecode(bytes_data, name=fullname,\n\u001b[32m    892\u001b[39m                                          bytecode_path=bytecode_path,\n\u001b[32m    893\u001b[39m                                          source_path=source_path)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     source_bytes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m code_object = \u001b[38;5;28mself\u001b[39m.source_to_code(source_bytes, source_path)\n\u001b[32m    897\u001b[39m _bootstrap._verbose_message(\u001b[33m'\u001b[39m\u001b[33mcode object from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m, source_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:953\u001b[39m, in \u001b[36mFileLoader.get_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the data from path as raw bytes.\"\"\"\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, (SourceLoader, ExtensionFileLoader)):\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_code\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m    954\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m file.read()\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/hubconf.py'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "YOLOV5_REPO = PROJECT_ROOT / \"yolov5_src\"\n",
    "model = torch.hub.load(str(YOLOV5_REPO), 'yolov5n', source='local')\n",
    "print(\"YOLOv5 model loaded successfully from\", YOLOV5_REPO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb63c6-9c1c-48e9-ae50-6aa025e5f027",
   "metadata": {},
   "source": [
    "## Checking GPU Availability and Details\n",
    "\n",
    "This cell checks the availability of CUDA and prints the number of GPUs available. For each available GPU, it also prints its name, helping to identify the GPUs accessible for running PyTorch operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12feeda-fc37-4e2f-b228-f13e27f58e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:31:43.993273Z",
     "iopub.status.busy": "2025-11-21T15:31:43.993158Z",
     "iopub.status.idle": "2025-11-21T15:31:43.995096Z",
     "shell.execute_reply": "2025-11-21T15:31:43.994730Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee4806-b5e7-490a-87d0-0aabf8d52093",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the YOLOv5 Model\n",
    "\n",
    "This cell runs the training script for the YOLOv5 model. Ensure the `data.yaml` file is correctly placed in the `yolov5` directory or provide the correct path to it. The training is configured with an image size of 640, batch size of 16, and runs for 10 epochs. The training weights are initialized with `yolov5n.pt`, and the process runs on device 0 (usually the first GPU). The results are saved in the `runs/train` directory under the name `exp_model-n_img-640`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6005ed-3be6-4ea8-b94a-3b3aa01ca082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:31:43.996075Z",
     "iopub.status.busy": "2025-11-21T15:31:43.996022Z",
     "iopub.status.idle": "2025-11-21T15:32:35.874303Z",
     "shell.execute_reply": "2025-11-21T15:32:35.873398Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/yolov5n.pt, cfg=, data=/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/dataset/data.yaml, hyp=../yolov5_src/data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=../yolov5_src/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train, name=exp_dummy, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-448-gdeec5e45 Python-3.14.0 torch-2.9.1 CPU\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\r\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train', view at http://localhost:6006/\r\n",
      "Overriding model.yaml nc=80 with nc=9\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \r\n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \r\n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \r\n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n",
      "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \r\n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \r\n",
      "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n",
      "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \r\n",
      "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \r\n",
      " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \r\n",
      " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \r\n",
      " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \r\n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \r\n",
      " 24      [17, 20, 23]  1     18942  models.yolo.Detect                      [9, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 214 layers, 1776094 parameters, 1776094 gradients, 4.3 GFLOPs\r\n",
      "\r\n",
      "Transferred 343/349 items from /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/yolov5n.pt\r\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Cl\u001b[0m\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Cl\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Clas\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Clas\u001b[0m\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.09 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\r\n",
      "Plotting labels to /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy/labels.jpg... \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 4 dataloader workers\r\n",
      "Logging results to \u001b[1m/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy\u001b[0m\r\n",
      "Starting training for 1 epochs...\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(amp):\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "        0/0         0G     0.1189      0.048    0.07033         16        640:  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "        0/0         0G     0.1189      0.048    0.07033         16        640:  /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(amp):\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "        0/0         0G     0.1206    0.04283    0.07646         10        640:  \r\n",
      "        0/0         0G     0.1206    0.04283    0.07646         10        640:  /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(amp):\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "        0/0         0G      0.124    0.04091    0.07747          9        640:  \r\n",
      "        0/0         0G      0.124    0.04091    0.07747          9        640: 1\r\n",
      "        0/0         0G      0.124    0.04091    0.07747          9        640: 1\r\n",
      "\r\n",
      "                 Class     Images  Instances          P          R      mAP50   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 0.700s exceeded\r\n",
      "\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all          4          9          0          0          0          0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "1 epochs completed in 0.002 hours.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy/weights/last.pt, 3.9MB\r\n",
      "Optimizer stripped from /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy/weights/best.pt, 3.9MB\r\n",
      "\r\n",
      "Validating /Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy/weights/best.pt...\r\n",
      "Fusing layers... \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 157 layers, 1771342 parameters, 0 gradients, 4.2 GFLOPs\r\n",
      "\r\n",
      "                 Class     Images  Instances          P          R      mAP50   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 0.700s exceeded\r\n",
      "\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all          4          9          0          0          0          0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1m/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5/runs/train/exp_dummy\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "YOLOV5_REPO = PROJECT_ROOT / \"yolov5_src\"\n",
    "DATA_YAML = PROJECT_ROOT / \"dataset\" / \"data.yaml\"\n",
    "WEIGHTS = YOLOV5_REPO / \"yolov5n.pt\"\n",
    "PROJECT_DIR = NOTEBOOK_DIR / \"runs\"\n",
    "RUN_NAME = \"exp_dummy\"\n",
    "\n",
    "!python {YOLOV5_REPO / 'train.py'} --img 640 --batch 4 --epochs 1 --data {DATA_YAML} --weights {WEIGHTS} --device cpu --project {PROJECT_DIR / 'train'} --name {RUN_NAME} --exist-ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c6cd8-2946-498c-be90-99fb615edcd5",
   "metadata": {},
   "source": [
    "## Object Detection and Visualization with YOLOv5\n",
    "\n",
    "This cell performs object detection on a set of test images using a custom-trained YOLOv5 model and visualizes the results with bounding boxes and labels.\n",
    "\n",
    "1. **Load the YOLOv5 Model**: The model is loaded from the specified path where the custom weights are stored.\n",
    "2. **Define a Function to Draw Bounding Boxes and Labels**: This function draws bounding boxes and labels on the detected objects in the image.\n",
    "3. **Specify Paths**: Set the paths for the directory containing the test images and the directory where the processed images will be saved.\n",
    "4. **Process Test Images**: \n",
    "    - List all image files in the test images directory.\n",
    "    - For each image, apply the YOLOv5 model to detect objects.\n",
    "    - Draw bounding boxes and labels on the detected objects.\n",
    "    - Save the processed images to the output directory.\n",
    "    - Optionally, display the processed images.\n",
    "\n",
    "The bounding boxes and labels are drawn using OpenCV, and the processed images are saved to the specified output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac716e82-6ca0-4b47-97c4-f79c84f7d56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:32:35.876811Z",
     "iopub.status.busy": "2025-11-21T15:32:35.876630Z",
     "iopub.status.idle": "2025-11-21T15:32:36.041729Z",
     "shell.execute_reply": "2025-11-21T15:32:36.041336Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TEST_IMAGES_DIR.exists():\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest images directory missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_IMAGES_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mYOLOV5_REPO\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWEIGHTS_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_boxes\u001b[39m(image, results):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m results.pred[\u001b[32m0\u001b[39m].detach().cpu().numpy():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:652\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    643\u001b[39m     repo_or_dir = _get_cache_or_reload(\n\u001b[32m    644\u001b[39m         repo_or_dir,\n\u001b[32m    645\u001b[39m         force_reload,\n\u001b[32m   (...)\u001b[39m\u001b[32m    649\u001b[39m         skip_validation=skip_validation,\n\u001b[32m    650\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m model = \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:682\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[32m    681\u001b[39m     hubconf_path = os.path.join(hubconf_dir, MODULE_HUBCONF)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     hub_module = \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m    685\u001b[39m     model = entry(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/.venv/lib/python3.14/site-packages/torch/hub.py:115\u001b[39m, in \u001b[36m_import_module\u001b[39m\u001b[34m(name, path)\u001b[39m\n\u001b[32m    113\u001b[39m module = importlib.util.module_from_spec(spec)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec.loader, Loader)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:758\u001b[39m, in \u001b[36m_LoaderBasics.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexec_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[32m    757\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the module.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    760\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcannot load module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m when \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    761\u001b[39m                           \u001b[33m'\u001b[39m\u001b[33mget_code() returns None\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:895\u001b[39m, in \u001b[36mSourceLoader.get_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n\u001b[32m    891\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _compile_bytecode(bytes_data, name=fullname,\n\u001b[32m    892\u001b[39m                                          bytecode_path=bytecode_path,\n\u001b[32m    893\u001b[39m                                          source_path=source_path)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     source_bytes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m code_object = \u001b[38;5;28mself\u001b[39m.source_to_code(source_bytes, source_path)\n\u001b[32m    897\u001b[39m _bootstrap._verbose_message(\u001b[33m'\u001b[39m\u001b[33mcode object from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m, source_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:953\u001b[39m, in \u001b[36mFileLoader.get_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the data from path as raw bytes.\"\"\"\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, (SourceLoader, ExtensionFileLoader)):\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_code\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m    954\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m file.read()\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/anuragdineshrokade/Documents/YOLO-Object-Detection-and-Classification-for-ADAS/yolov5_src/hubconf.py'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "YOLOV5_REPO = PROJECT_ROOT / \"yolov5_src\"\n",
    "RUN_DIR = NOTEBOOK_DIR / \"runs\" / \"train\" / \"exp_dummy\" / \"weights\"\n",
    "WEIGHTS_PATH = (RUN_DIR / \"best.pt\") if (RUN_DIR / \"best.pt\").exists() else (RUN_DIR / \"last.pt\")\n",
    "TEST_IMAGES_DIR = PROJECT_ROOT / \"dataset\" / \"images\" / \"val\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output_images_test_set\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if not WEIGHTS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected weights not found in {RUN_DIR}\")\n",
    "\n",
    "if not TEST_IMAGES_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Test images directory missing: {TEST_IMAGES_DIR}\")\n",
    "\n",
    "model = torch.hub.load(str(YOLOV5_REPO), 'custom', path=str(WEIGHTS_PATH), source='local', force_reload=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, results):\n",
    "    for pred in results.pred[0].detach().cpu().numpy():\n",
    "        x1, y1, x2, y2, conf, cls = pred\n",
    "        label = f\"{model.names[int(cls)]} {conf:.2f}\"\n",
    "        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(image, (int(x1), int(y1) - 20), (int(x1) + w, int(y1)), (0, 255, 0), -1)\n",
    "        cv2.putText(image, label, (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(TEST_IMAGES_DIR) if f.lower().endswith(('.jpg', '.png'))])\n",
    "print(f\"Processing {len(image_files)} images from {TEST_IMAGES_DIR}\")\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = TEST_IMAGES_DIR / image_file\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Skipping unreadable file: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    results = model(image)\n",
    "    draw_boxes(image, results)\n",
    "\n",
    "    output_image_path = OUTPUT_DIR / image_file\n",
    "    cv2.imwrite(str(output_image_path), image)\n",
    "    print(f\"Saved detections to {output_image_path}\")\n",
    "\n",
    "print(f\"All processed images saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7429156-d8fe-448d-a23a-72add1058c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
